{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to load the api key from file\n",
    "secrets_filename = os.path.join(os.path.abspath(\n",
    "    # replace values here if this was cloned\n",
    "    os.path.dirname('deser.ipynb')), 'secret2.txt')\n",
    "try:\n",
    "    with open(secrets_filename, 'r') as f:\n",
    "        api_key = f.read().strip()\n",
    "        openai.api_key = api_key\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_list(entity: str, features, type_of=type(list), n=5):\n",
    "    artifacts = ['...',r'\\n']\n",
    "    if type(type_of) != type(list()):\n",
    "        pass\n",
    "        # print('type other than list passed')\n",
    "    # delimiter = \"|\"\n",
    "\n",
    "    if type(type_of) != type(dict()):\n",
    "        clean_format_name = '.json file'\n",
    "    else:\n",
    "        clean_format_name = 'Python List'\n",
    "        \n",
    "    role_context = \"\"\"You are a system that processes data. \n",
    "    You make assumptions instead of asking for clarification. \n",
    "    Your result should contain no explanation or comments. \n",
    "    Your result should be valid json that can be serialized.\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Create an list of values based on a prompt.\n",
    "    Limit your result to {n} records of data. \n",
    "    The parent json should have only one field: data. \n",
    "    The data field contains a list of values.\n",
    "    Do not ask for clarification. Make inferences. \n",
    "    If a record will be cut off, do not include it in the output.\n",
    "    # Format your results as {clean_format_name}.\n",
    "    No comments in the code.\n",
    "    \"\"\"\n",
    "    prompt += r' Use this example {\"data\":[{\"name\":\"item1\",\"weight\":2},{\"name\":\"item2\",\"weight\":1}]}'\n",
    "\n",
    "    # add specific entity and features \n",
    "    prompt += f\" for entity=\\\"{entity}\\\"\"\n",
    "    feature_str = ', '.join(features)\n",
    "    prompt += f\" having features=\\\"{entity}, {feature_str}\\\"\"\n",
    "\n",
    "    # only the messages have been changed\n",
    "    res = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"system\", \"content\": role_context},\n",
    "                  {\"role\": \"system\", \"content\": prompt}\n",
    "                  ],\n",
    "        temperature=0.8,\n",
    "        max_tokens=1024,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        timeout=30,\n",
    "        presence_penalty=0.5,\n",
    "        frequency_penalty=0.5\n",
    "    )\n",
    "\n",
    "    # select choices from the response\n",
    "    choices = res['choices']\n",
    "    if len(choices) != 1:\n",
    "        print('')\n",
    "    for choice in choices:\n",
    "        res_ser = choice['message']['content']\n",
    "        # return lists\n",
    "        if type_of == type(list()):\n",
    "            res_list = res_ser.split('|')\n",
    "            return res_list[:n]\n",
    "        else:\n",
    "            # remove chat's text artifacts before deserializing\n",
    "            for artifact in artifacts:\n",
    "                res_ser = res_ser.replace(artifact, '')\n",
    "            data = json.loads(res_ser)['data']\n",
    "            # return as a pd data frame\n",
    "            return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_scatter_plot(entity, features, n=5):\n",
    "    entity = entity.replace(\" \", \"_\")\n",
    "    features = [feature.replace(\" \", \"_\") for feature in features]\n",
    "    entity_list = get_entity_list(entity=entity, features=features, type_of=type(dict), n=5)\n",
    "\n",
    "    df = pd.DataFrame(entity_list)\n",
    "\n",
    "    x_vals = features[0]\n",
    "    y_vals = features[1]\n",
    "\n",
    "    # check types and change behavior accordingly\n",
    "    for t in df.dtypes:\n",
    "        pass\n",
    "        # print(t)\n",
    "\n",
    "    # create a scatter plot for the x and y features\n",
    "    ax = df.plot.scatter(x=x_vals, y=y_vals)\n",
    "\n",
    "    # add data labels\n",
    "    for i, txt in enumerate(df[entity]):\n",
    "        ax.annotate(txt, (df[x_vals][i], df[y_vals][i]))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tries_ctr = 0\n",
    "max_tries = 3\n",
    "while tries_ctr < max_tries:\n",
    "    try:\n",
    "        print(get_entity_list(\n",
    "            entity=\"Great Lakes\", \n",
    "            features=[\"Deepest Point (ft.)\", \"Area Sq. Mi.\"]\n",
    "        ))\n",
    "\n",
    "        entity_scatter_plot(\n",
    "            entity=\"nation\", \n",
    "            features=[\"rank education system\", \"rank healthcare system\"]\n",
    "        )\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
